{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Pinecone in module langchain_community.vectorstores.pinecone:\n",
      "\n",
      "class Pinecone(langchain_core.vectorstores.VectorStore)\n",
      " |  Pinecone(index: 'Any', embedding: 'Union[Embeddings, Callable]', text_key: 'str', namespace: 'Optional[str]' = None, distance_strategy: 'Optional[DistanceStrategy]' = <DistanceStrategy.COSINE: 'COSINE'>)\n",
      " |  \n",
      " |  `Pinecone` vector store.\n",
      " |  \n",
      " |  To use, you should have the ``pinecone-client`` python package installed.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain_community.vectorstores import Pinecone\n",
      " |          from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
      " |          import pinecone\n",
      " |  \n",
      " |          # The environment should be the one specified next to the API key\n",
      " |          # in your Pinecone console\n",
      " |          pinecone.init(api_key=\"***\", environment=\"...\")\n",
      " |          index = pinecone.Index(\"langchain-demo\")\n",
      " |          embeddings = OpenAIEmbeddings()\n",
      " |          vectorstore = Pinecone(index, embeddings.embed_query, \"text\")\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Pinecone\n",
      " |      langchain_core.vectorstores.VectorStore\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, index: 'Any', embedding: 'Union[Embeddings, Callable]', text_key: 'str', namespace: 'Optional[str]' = None, distance_strategy: 'Optional[DistanceStrategy]' = <DistanceStrategy.COSINE: 'COSINE'>)\n",
      " |      Initialize with Pinecone client.\n",
      " |  \n",
      " |  add_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, namespace: 'Optional[str]' = None, batch_size: 'int' = 32, embedding_chunk_size: 'int' = 1000, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more texts through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Upsert optimization is done by chunking the embeddings and upserting them.\n",
      " |      This is done to avoid memory issues and optimize using HTTP based embeddings.\n",
      " |      For OpenAI embeddings, use pool_threads>4 when constructing the pinecone.Index,\n",
      " |      embedding_chunk_size>1000 and batch_size~64 for best performance.\n",
      " |      Args:\n",
      " |          texts: Iterable of strings to add to the vectorstore.\n",
      " |          metadatas: Optional list of metadatas associated with the texts.\n",
      " |          ids: Optional list of ids to associate with the texts.\n",
      " |          namespace: Optional pinecone namespace to add the texts to.\n",
      " |          batch_size: Batch size to use when adding the texts to the vectorstore.\n",
      " |          embedding_chunk_size: Chunk size to use when embedding the texts.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of ids from adding the texts into the vectorstore.\n",
      " |  \n",
      " |  delete(self, ids: 'Optional[List[str]]' = None, delete_all: 'Optional[bool]' = None, namespace: 'Optional[str]' = None, filter: 'Optional[dict]' = None, **kwargs: 'Any') -> 'None'\n",
      " |      Delete by vector IDs or filter.\n",
      " |      Args:\n",
      " |          ids: List of ids to delete.\n",
      " |          filter: Dictionary of conditions to filter vectors to delete.\n",
      " |  \n",
      " |  max_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, filter: 'Optional[dict]' = None, namespace: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      \n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  max_marginal_relevance_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, filter: 'Optional[dict]' = None, namespace: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      \n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          embedding: Embedding to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  similarity_search(self, query: 'str', k: 'int' = 4, filter: 'Optional[dict]' = None, namespace: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return pinecone documents most similar to query.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          filter: Dictionary of argument(s) to filter on metadata\n",
      " |          namespace: Namespace to search in. Default will search in '' namespace.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents most similar to the query and score for each\n",
      " |  \n",
      " |  similarity_search_by_vector_with_score(self, embedding: 'List[float]', *, k: 'int' = 4, filter: 'Optional[dict]' = None, namespace: 'Optional[str]' = None) -> 'List[Tuple[Document, float]]'\n",
      " |      Return pinecone documents most similar to embedding, along with scores.\n",
      " |  \n",
      " |  similarity_search_with_score(self, query: 'str', k: 'int' = 4, filter: 'Optional[dict]' = None, namespace: 'Optional[str]' = None) -> 'List[Tuple[Document, float]]'\n",
      " |      Return pinecone documents most similar to query, along with scores.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          filter: Dictionary of argument(s) to filter on metadata\n",
      " |          namespace: Namespace to search in. Default will search in '' namespace.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents most similar to the query and score for each\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_existing_index(index_name: 'str', embedding: 'Embeddings', text_key: 'str' = 'text', namespace: 'Optional[str]' = None, pool_threads: 'int' = 4) -> 'Pinecone' from abc.ABCMeta\n",
      " |      Load pinecone vectorstore from index name.\n",
      " |  \n",
      " |  from_texts(texts: 'List[str]', embedding: 'Embeddings', metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, batch_size: 'int' = 32, text_key: 'str' = 'text', namespace: 'Optional[str]' = None, index_name: 'Optional[str]' = None, upsert_kwargs: 'Optional[dict]' = None, pool_threads: 'int' = 4, embeddings_chunk_size: 'int' = 1000, **kwargs: 'Any') -> 'Pinecone' from abc.ABCMeta\n",
      " |      Construct Pinecone wrapper from raw documents.\n",
      " |      \n",
      " |      This is a user friendly interface that:\n",
      " |          1. Embeds documents.\n",
      " |          2. Adds the documents to a provided Pinecone index\n",
      " |      \n",
      " |      This is intended to be a quick way to get started.\n",
      " |      \n",
      " |      The `pool_threads` affects the speed of the upsert operations.\n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_community.vectorstores import Pinecone\n",
      " |              from langchain_community.embeddings import OpenAIEmbeddings\n",
      " |              import pinecone\n",
      " |      \n",
      " |              # The environment should be the one specified next to the API key\n",
      " |              # in your Pinecone console\n",
      " |              pinecone.init(api_key=\"***\", environment=\"...\")\n",
      " |              embeddings = OpenAIEmbeddings()\n",
      " |              pinecone = Pinecone.from_texts(\n",
      " |                  texts,\n",
      " |                  embeddings,\n",
      " |                  index_name=\"langchain-demo\"\n",
      " |              )\n",
      " |  \n",
      " |  get_pinecone_index(index_name: 'Optional[str]', pool_threads: 'int' = 4) -> 'Index' from abc.ABCMeta\n",
      " |      Return a Pinecone Index instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name: Name of the index to use.\n",
      " |          pool_threads: Number of threads to use for index upsert.\n",
      " |      Returns:\n",
      " |          Pinecone Index instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  embeddings\n",
      " |      Access the query embedding object if available.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  async aadd_documents(self, documents: 'List[Document]', **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more documents through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (List[Document]: Documents to add to the vectorstore.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  async aadd_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[List[dict]]' = None, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more texts through the embeddings and add to the vectorstore.\n",
      " |  \n",
      " |  add_documents(self, documents: 'List[Document]', **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more documents through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (List[Document]: Documents to add to the vectorstore.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  async adelete(self, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Optional[bool]'\n",
      " |      Delete by vector ID or other criteria.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: List of ids to delete.\n",
      " |          **kwargs: Other keyword arguments that subclasses might use.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Optional[bool]: True if deletion is successful,\n",
      " |          False otherwise, None if not implemented.\n",
      " |  \n",
      " |  async amax_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |  \n",
      " |  async amax_marginal_relevance_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |  \n",
      " |  as_retriever(self, **kwargs: 'Any') -> 'VectorStoreRetriever'\n",
      " |      Return VectorStoreRetriever initialized from this VectorStore.\n",
      " |      \n",
      " |      Args:\n",
      " |          search_type (Optional[str]): Defines the type of search that\n",
      " |              the Retriever should perform.\n",
      " |              Can be \"similarity\" (default), \"mmr\", or\n",
      " |              \"similarity_score_threshold\".\n",
      " |          search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
      " |              search function. Can include things like:\n",
      " |                  k: Amount of documents to return (Default: 4)\n",
      " |                  score_threshold: Minimum relevance threshold\n",
      " |                      for similarity_score_threshold\n",
      " |                  fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
      " |                  lambda_mult: Diversity of results returned by MMR;\n",
      " |                      1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
      " |                  filter: Filter by document metadata\n",
      " |      \n",
      " |      Returns:\n",
      " |          VectorStoreRetriever: Retriever class for VectorStore.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # Retrieve more documents with higher diversity\n",
      " |          # Useful if your dataset has many similar documents\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"mmr\",\n",
      " |              search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
      " |          )\n",
      " |      \n",
      " |          # Fetch more documents for the MMR algorithm to consider\n",
      " |          # But only return the top 5\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"mmr\",\n",
      " |              search_kwargs={'k': 5, 'fetch_k': 50}\n",
      " |          )\n",
      " |      \n",
      " |          # Only retrieve documents that have a relevance score\n",
      " |          # Above a certain threshold\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"similarity_score_threshold\",\n",
      " |              search_kwargs={'score_threshold': 0.8}\n",
      " |          )\n",
      " |      \n",
      " |          # Only get the single most similar document from the dataset\n",
      " |          docsearch.as_retriever(search_kwargs={'k': 1})\n",
      " |      \n",
      " |          # Use a filter to only retrieve documents from a specific paper\n",
      " |          docsearch.as_retriever(\n",
      " |              search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
      " |          )\n",
      " |  \n",
      " |  async asearch(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query using specified search type.\n",
      " |  \n",
      " |  async asimilarity_search(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query.\n",
      " |  \n",
      " |  async asimilarity_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to embedding vector.\n",
      " |  \n",
      " |  async asimilarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs and relevance scores in the range [0, 1], asynchronously.\n",
      " |      \n",
      " |      0 is dissimilar, 1 is most similar.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: input text\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
      " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
      " |                  filter the resulting set of retrieved docs\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Tuples of (doc, similarity_score)\n",
      " |  \n",
      " |  async asimilarity_search_with_score(self, *args: 'Any', **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Run similarity search with distance asynchronously.\n",
      " |  \n",
      " |  search(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query using specified search type.\n",
      " |  \n",
      " |  similarity_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to embedding vector.\n",
      " |      \n",
      " |      Args:\n",
      " |          embedding: Embedding to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents most similar to the query vector.\n",
      " |  \n",
      " |  similarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs and relevance scores in the range [0, 1].\n",
      " |      \n",
      " |      0 is dissimilar, 1 is most similar.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: input text\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
      " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
      " |                  filter the resulting set of retrieved docs\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Tuples of (doc, similarity_score)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  async afrom_documents(documents: 'List[Document]', embedding: 'Embeddings', **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from documents and embeddings.\n",
      " |  \n",
      " |  async afrom_texts(texts: 'List[str]', embedding: 'Embeddings', metadatas: 'Optional[List[dict]]' = None, **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from texts and embeddings.\n",
      " |  \n",
      " |  from_documents(documents: 'List[Document]', embedding: 'Embeddings', **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from documents and embeddings.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl.embeddings import push_vectors_from_docs, get_vectore_store_for_query, delete_all_vectors_from_db\n",
    "from etl.text_splitter import create_text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pdfs...\n",
      "loading loaded...\n",
      "loading words...\n",
      "words loaded...\n",
      "cleaning texts...\n",
      "text cleaned...\n"
     ]
    }
   ],
   "source": [
    "docs = create_text_chunks('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = await get_vectore_store_for_query(index_name = 'mvp-test3', model_embedding = 'hf', vector_store_client = 'pinecone', namespace = 'hf-384')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing the embedding model...\n",
      "Choosing the vector store client...\n"
     ]
    }
   ],
   "source": [
    "a = await push_vectors_from_docs(index_name = 'mvp-test3', docs = docs[40:50], model_embedding = 'hf', vector_store_client = 'pinecone', namespace = 'hf-384-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing the embedding model...\n",
      "Choosing the vector store client...\n"
     ]
    }
   ],
   "source": [
    "b = await delete_all_vectors_from_db(index_name = 'mvp-test3', vector_store_client = 'pinecone', namespace = 'hf-384-2', model_embedding='hf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
